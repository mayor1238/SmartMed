# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/176wjy_JkoJ27Flyihk7oScttpib42yeh
"""

import requests
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "distilbert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

from datasets import load_dataset, Dataset, Features, Value
from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer
from transformers import AutoModelForSequenceClassification
import pandas as pd # Import pandas for data manipulation
from sklearn.model_selection import train_test_split

# Load the CSV into a pandas DataFrame first
df = pd.read_csv("/content/SmartMed NG - Sheet1 (1).csv")

# TODO: Identify your text column and label column
# The previous preprocess function used 'symptoms', so we'll assume that's the text column.
text_column_name = "symptoms"

# You MUST replace 'Your_Label_Column_Name' with the actual name of the column
# in your CSV file that contains the categorical labels (e.g., 'diagnosis', 'condition').
label_column_name = "label"

# Map string labels to integers
unique_labels = df[label_column_name].unique()
label_to_id = {label: i for i, label in enumerate(unique_labels)}
id_to_label = {i: label for label, i in label_to_id.items()} # Optional, for decoding predictions later

df["label"] = df[label_column_name].map(label_to_id)

# Convert pandas DataFrame to a Hugging Face Dataset
# Before converting to Dataset, define features to ensure 'label' is an `int` type
features = Features({
    'symptoms': Value('string'),
    'duration': Value('string'),
    'severity': Value('string'),
    'label': Value('int32'),    # Ensure 'label' is an integer type
})

# Split the DataFrame into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])

# Convert pandas DataFrames to Hugging Face Datasets
# Reset index to avoid __index_level_0__ column in the dataset conversion
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True), features=features)
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True), features=features)

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def preprocess_function(examples):
    # Tokenize the text column
    tokenized_inputs = tokenizer(examples[text_column_name], truncation=True)
    # Add the numerical labels
    tokenized_inputs["labels"] = examples["label"]
    return tokenized_inputs

encoded_train_dataset = train_dataset.map(preprocess_function, batched=True)
encoded_test_dataset = test_dataset.map(preprocess_function, batched=True)

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=len(unique_labels) # Set num_labels dynamically based on your unique labels
)

# Define data collator for dynamic padding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

args = TrainingArguments(
    output_dir="triage_model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    num_train_epochs=4,
    eval_strategy="epoch",
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=encoded_train_dataset,
    eval_dataset=encoded_test_dataset,  # Evaluate on the test dataset
    tokenizer=tokenizer, # Pass tokenizer to Trainer for data collator
    data_collator=data_collator # Use DataCollatorWithPadding
)

trainer.train()

model.save_pretrained("smartmed-triage-model")
tokenizer.save_pretrained("smartmed-triage-model")

!zip -r smartmed-triage-model.zip smartmed-triage-model

from transformers import pipeline

clf = pipeline("text-classification", model="smartmed-triage-model", tokenizer="smartmed-triage-model")

clf("I have chest pain and difficulty breathing")



# Evaluate the model on the test dataset
results = trainer.evaluate(encoded_test_dataset)
print(results)

# Get predictions
predictions = trainer.predict(encoded_test_dataset)
preds = predictions.predictions.argmax(-1)

# Import necessary metrics
from sklearn.metrics import classification_report, confusion_matrix

# Map predictions to original labels
predicted_labels = [id_to_label[p] for p in preds]
true_labels = [id_to_label[l] for l in encoded_test_dataset["labels"]]

# Print classification report
print("\nClassification Report:")
print(classification_report(true_labels, predicted_labels, target_names=list(unique_labels)))

# Print confusion matrix
print("\nConfusion Matrix:")
print(confusion_matrix(true_labels, predicted_labels))









"""# Task
Create a Flask microservice with an `/predict` endpoint that uses the trained `smartmed-triage-model` to classify patient symptoms. The service should load the model and its tokenizer, accept patient symptoms as a POST request, return the predicted label and score, and include instructions on how to install dependencies and run the application locally.

## Create Flask App Files

### Subtask:
Generate the necessary files for a Flask application, including `app.py` for the service logic and `requirements.txt` for dependencies.

**Reasoning**:
The first step is to create the `app.py` file with the initial Flask application setup and necessary imports as instructed.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from flask import Flask, request, jsonify
# from transformers import pipeline
# 
# app = Flask(__name__)
# 
# # Placeholder for model loading, will be added in subsequent steps
# # clf = pipeline("text-classification", model="./smartmed-triage-model", tokenizer="./smartmed-triage-model")
# 
# @app.route('/')
# def home():
#     return 'Flask app is running!'
# 
# if __name__ == '__main__':
#     app.run(debug=True, host='0.0.0.0', port=5000)

"""**Reasoning**:
The next step is to create the `requirements.txt` file listing the required Python packages for the Flask application as specified in the instructions.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# Flask
# transformers
# torch

"""## Load Model and Tokenizer

### Subtask:
In `app.py`, add code to load the trained `smartmed-triage-model` and its tokenizer using the `pipeline` function from the `transformers` library.

**Reasoning**:
The subtask requires modifying `app.py` to load the `id_to_label` dictionary and uncomment the model pipeline initialization. I will rewrite the `app.py` content to include the `id_to_label` dictionary directly and activate the model loading.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from flask import Flask, request, jsonify
# from transformers import pipeline
# 
# app = Flask(__name__)
# 
# id_to_label = {"0": "self_care", "1": "Appointment", "2": "Urgent", "3": "Emergency"} # Map numerical labels back to descriptive names
# 
# # Load the text classification pipeline
# clf = pipeline("text-classification", model="./smartmed-triage-model", tokenizer="./smartmed-triage-model")
# 
# @app.route('/')
# def home():
#     return 'Flask app is running!'
# 
# if __name__ == '__main__':
#     app.run(debug=True, host='0.0.0.0', port=5000)

"""**Reasoning**:
The previous `id_to_label` dictionary in `app.py` was created with string keys, but the `id_to_label` from the kernel state uses integer keys. To ensure consistency and correct mapping of numerical predictions, I will rewrite `app.py` to use integer keys for the `id_to_label` dictionary.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from flask import Flask, request, jsonify
# from transformers import pipeline
# 
# app = Flask(__name__)
# 
# id_to_label = {0: "self_care", 1: "Appointment", 2: "Urgent", 3: "Emergency"} # Corrected to use integer keys
# 
# # Load the text classification pipeline
# clf = pipeline("text-classification", model="./smartmed-triage-model", tokenizer="./smartmed-triage-model")
# 
# @app.route('/')
# def home():
#     return 'Flask app is running!'
# 
# if __name__ == '__main__':
#     app.run(debug=True, host='0.0.0.0', port=5000)

"""## Define Prediction Endpoint

### Subtask:
Implement a Flask route (e.g., `/predict`) that accepts patient symptoms via a POST request, uses the loaded model to make a prediction, and returns the predicted label and score as a JSON response.

**Reasoning**:
The subtask requires implementing a `/predict` endpoint in `app.py`. I will add a new route that accepts POST requests, extracts symptoms, uses the model for prediction, maps the numerical label to its descriptive name, and returns a JSON response.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from flask import Flask, request, jsonify
# from transformers import pipeline
# 
# app = Flask(__name__)
# 
# id_to_label = {0: "self_care", 1: "Appointment", 2: "Urgent", 3: "Emergency"}
# 
# # Load the text classification pipeline
# clf = pipeline("text-classification", model="./smartmed-triage-model", tokenizer="./smartmed-triage-model")
# 
# @app.route('/')
# def home():
#     return 'Flask app is running!'
# 
# @app.route('/predict', methods=['POST'])
# def predict():
#     if not request.json or 'symptoms' not in request.json:
#         return jsonify({"error": "Invalid request body. Please provide 'symptoms' in JSON format."}), 400
# 
#     symptoms = request.json['symptoms']
# 
#     # Make prediction
#     prediction = clf(symptoms)
# 
#     # Extract predicted label and score
#     # The prediction output is typically a list of dicts, e.g., [{'label': 'LABEL_X', 'score': Y.Z}]
#     predicted_label_str = prediction[0]['label'] # e.g., 'LABEL_1'
#     score = prediction[0]['score']
# 
#     # Convert 'LABEL_X' to integer X
#     predicted_label_id = int(predicted_label_str.split('_')[1])
# 
#     # Map numerical ID to descriptive label
#     descriptive_label = id_to_label.get(predicted_label_id, 'Unknown')
# 
#     return jsonify({"prediction": descriptive_label, "score": score})
# 
# if __name__ == '__main__':
#     app.run(debug=True, host='0.0.0.0', port=5000)

"""### `app.py` Content"""

from flask import Flask, request, jsonify
from transformers import pipeline

app = Flask(__name__)

id_to_label = {0: "self_care", 1: "Appointment", 2: "Urgent", 3: "Emergency"}

# Load the text classification pipeline
clf = pipeline("text-classification", model="./smartmed-triage-model", tokenizer="./smartmed-triage-model")

@app.route('/')
def home():
    return 'Flask app is running!'

@app.route('/predict', methods=['POST'])
def predict():
    if not request.json or 'symptoms' not in request.json:
        return jsonify({"error": "Invalid request body. Please provide 'symptoms' in JSON format."}), 400

    symptoms = request.json['symptoms']

    # Make prediction
    prediction = clf(symptoms)

    # Extract predicted label and score
    # The prediction output is typically a list of dicts, e.g., [{'label': 'LABEL_X', 'score': Y.Z}]
    predicted_label_str = prediction[0]['label'] # e.g., 'LABEL_1'
    score = prediction[0]['score']

    # Convert 'LABEL_X' to integer X
    predicted_label_id = int(predicted_label_str.split('_')[1])

    # Map numerical ID to descriptive label
    descriptive_label = id_to_label.get(predicted_label_id, 'Unknown')

    return jsonify({"prediction": descriptive_label, "score": score})

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)

"""### `requirements.txt` Content"""

Flask
transformers
torch

"""Remember that you'll also need to include the `smartmed-triage-model` directory (which contains the saved model and tokenizer files) in your repository alongside `app.py` and `requirements.txt` for the Flask application to run correctly. This directory was created by running the `model.save_pretrained` and `tokenizer.save_pretrained` commands earlier.

## Provide Running Instructions

### Subtask:
Generate instructions on how to install dependencies and run the Flask application locally.

### Running Instructions

To run the Flask application locally, follow these steps:

1.  **Install Dependencies**:
    Navigate to the directory containing `requirements.txt` and `app.py` in your terminal or command prompt. Then, install the required Python packages using `pip`:

    ```bash
    pip install -r requirements.txt
    ```

2.  **Run the Flask Application**:
    After installing the dependencies, you can start the Flask development server by running `app.py`:

    ```bash
    python app.py
    ```

    The application will typically run on `http://0.0.0.0:5000` or `http://127.0.0.1:5000`. You will see output in your terminal indicating that the Flask server is running.

3.  **Test the `/predict` Endpoint**:
    You can test the `/predict` endpoint using a tool like `curl` or Postman. For example, using `curl`:

    ```bash
    curl -X POST -H "Content-Type: application/json" -d '{"symptoms": "I have a headache and fever."}' http://127.0.0.0:5000/predict
    ```

    This will send a POST request with sample symptoms to your Flask application, and you should receive a JSON response with the predicted label and score.
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Final Task

### Subtask:
Summarize the steps taken to create the Flask microservice and explain how to use it.

## Summary:

### Q&A

**How was the Flask microservice created and how can it be used?**
The Flask microservice was created by setting up an `app.py` file for the application logic and a `requirements.txt` for dependencies. The `smartmed-triage-model` and its tokenizer were loaded using the `transformers` library's `pipeline` function. A `/predict` endpoint was implemented to accept patient symptoms via a POST request, classify them using the loaded model, and return the predicted label and score in a JSON response.

To use the microservice:
1.  **Install dependencies**: Navigate to the directory containing `app.py` and `requirements.txt`, then run `pip install -r requirements.txt`.
2.  **Run the Flask application**: Execute `python app.py` in the terminal. The service will run on `http://0.0.0.0:5000` or `http://127.0.0.1:5000`.
3.  **Test the `/predict` endpoint**: Send a POST request to `http://127.0.0.0:5000/predict` with a JSON body containing patient symptoms, for example: `curl -X POST -H "Content-Type: application/json" -d '{"symptoms": "I have a headache and fever."}' http://127.0.0.0:5000/predict`.

### Data Analysis Key Findings

*   The Flask application structure was initialized with `app.py` and `requirements.txt`, specifying `Flask`, `transformers`, and `torch` as dependencies.
*   The `smartmed-triage-model` and its tokenizer were successfully loaded into a text classification pipeline using `transformers.pipeline`.
*   A mapping dictionary, `id_to_label`, was defined to convert numerical model outputs (e.g., `0`) into descriptive labels (e.g., "self\_care").
*   A `/predict` endpoint was implemented to handle POST requests, validate input for 'symptoms', make predictions using the loaded model, map numerical labels to descriptive ones, and return a JSON response containing the `prediction` (e.g., "self\_care") and its `score`.
*   The instructions clearly outline how to install dependencies (`pip install -r requirements.txt`), run the Flask application (`python app.py`), and test the `/predict` endpoint using `curl`.

### Insights or Next Steps

*   **Deployment**: Consider containerizing the Flask microservice using Docker for easier deployment and scaling in production environments.
*   **Error Handling and Logging**: Enhance the error handling for the `/predict` endpoint with more specific error messages and implement logging to monitor application performance and issues.
"""